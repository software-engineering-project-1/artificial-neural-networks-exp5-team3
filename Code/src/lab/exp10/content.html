<!-- This file needs to be edited by the lab developer to suit
the requirements of their lab in particular.-->

<!-- Add class="default" to include any element as it is
specified in default.html. 
Do not include class="default" to the elements that you want to
edit -->

<!DOCTYPE html>
<html>
<head> <title>Artificial Neural Networks </title> </head>
<body>

<div id="experiment"> <!-- The Experiment Document Container-->

  <!-- The lab Header contains the logo and the name of the lab,
  usually displayed on the top of the page-->

  <header id="experiment-header" class="default">
  
    <div id="experiment-header-logo" class="logo">
      <!-- Enclose the logo image of your lab or write it in 
      text-->
      <!-- <img src="../images/logo.jpg" /> -->

    </div>

    <div id="experiment-header-heading" class="heading">
      <!-- Write the name of your lab and link it to the home 
      page of your lab (h1 tag is preferred while writing your 
      lab name)-->
      <a href="../index.html">Artificial Neural Networks Virtual Lab</a>	
    </div>

    <!-- Add any additional element you want to add to the lab 
    header, For example : Help (Enclosing them with suitable 
    div is recommended)-->

  </header>


  <!-- The lab article is the main content area where all the 
  experiment content sits-->
  <article id="experiment-article">
  
    <!-- The lab article has an header, optional navigational 
    menu, number of sections, an optional sidebar and a closing 
    footer-->
    
      <header id="experiment-article-heading" class="heading">
        <!-- You can add a welcome message or title of the 
        experiment here -->
        Deterministic, stochastic and mean-field annealing of Hopfield models
        <!-- Add any additional element if required with proper 
        enclosing-->
      </header>

      <!-- Navigation menu is useful to organize the view of 
      multiple sections inside the article-->
      <nav id="experiment-article-navigation" class="default">
        <ul id="experiment-article-navigation-menu">
          <!-- The menu can be dynamically generated to contain 
          the headings of your sections or instead write the 
          menu items of your choice individually enclosedu in 
          <li> tag as shown below-->
        </ul>
      </nav>

      <!-- All the sections of your lab or experiment can be 
      enclosed together with a div element as shown below-->
      <div id="experiment-article-sections">

 <!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->
        <!-- First section of the article-->
        <section id="experiment-article-section-1">
          
          <div id="experiment-article-section-1-icon" 
          class="icon">
	    <!-- Enclose the icon image of your lab -->
	    <img src="../images/objective.jpg" />
	  </div>	
          
          <!-- The heading for the section can be enclosed in a 
          div tag. -->
          <div id="experiment-article-section-1-heading" 
          class="heading">
            Objective
          </div>

          <!-- Write the section content inside a paragraph 
          element, You can also include images with <img> tag -->
          <div id="experiment-article-section-1-content" 
          class="content">	
            <p>
<P>The objective of this experiment is to demonstrate different annealing strategies in the solutions to optimization problems using a
Hopfield model. This is illustrated using the weighted matching problem as a case study.<br><br>
The optimization is guided by the activation dynamics of a Hopfield network.
The activation dynamics is guided by the energy surface defined by the activation states of the units in the Hopfield network.
The three relaxation strategies studied are:<br>
(a) Deterministic relaxation<br>
(b) Stochastic relaxation <br> 
(c) Mean-field approximation<br><br>

<P><BR><BR>
            </p>
        </div>


      </section>

      <!-- Second section of the article-->


 <!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->
      </section>

      <section id="experiment-article-section-2">
   
        <div id="experiment-article-section-2-icon" 
        class="icon">
	  <!-- Enclose the icon image of your lab.-->
	<img src="../images/theory.jpg" />
	</div>

        <div id="experiment-article-section-2-heading" 
        class="heading">
         Tutorial 
        </div>

        <div id="experiment-article-section-2-content" 
        class="content">

<h3>Optimization</h3>
<P>
One of the most successful applications of neural network principles 
is in solving optimization problems. There
are many situations where a problem can be formulated as 
minimization or maximization of some cost function or objective 
function subject to certain constraints. It is possible to map such a 
problem onto a feedback network, where the units and connection
strengths are identiﬁed by comparing the cost function of the problem 
with the energy function of the network expressed in terms of the 
state values of the units and the connection strengths. The solution 
to the problem lies in determining the state of the network at the 
global minimum of the energy function. In this process, it is necessary 
to overcome the local minima of the energy function. This is accomplished 
by adopting a simulated annealing schedule for implementing 
the search for global minimum [Kirkpatric et al,1983]. 
</P>
<P>The solution to an optimization problem by neural networks 
consists of the following steps:
<OL><OL>(a) Express the objective function or the cost function and the constraints 
of the given problem in terms of the variables of the problem:
<OL><OL><br> \(Objective\) \(function ~ (E)\) \(:\) \(cost+global~constraints\) 
</OL></OL><br>
(b) Compare the objective function in above equation with the energy
function (given in equation below) of a feedback neural network of Hopﬁeld type to
identify the states and the weights of the network in terms of variables and parameters appearing in the objective function.
<OL><OL>
<br>\(Energy\) \(function: E = -\frac{1}{2}\sum\limits_{i\ne j} w_{ij}s_is_j \qquad(1) \) <br>
	\(w_{ij}\) : weight connecting unit \(j\) to unit \(i\) <br>
	\(s_i, s_j\) : state values
</OL></OL>
</OL></OL><br>
<OL><OL>
(c) The solution to the optimization problem consists of determining 
the state corresponding to the global minimum of the energy function 
of the network. Assuming bipolar states for each unit, the
dynamics of the network can be expressed as
<OL><OL><br> \(s_i(t+ 1) =  sgn(\sum\limits_{j\ne i}w_{ij}s_j(t))\) , for \(i=1,2, .. ,N \qquad(2)\) <br>
where \(N\) is the number of units in the network.
</OL></OL><br>
</OL></OL>
<OL><OL>
(d) Direct application of the above dynamics in search of a stable
state may lead to a state corresponding to a local minimum of the 
energy function. In order to reach the global minimum, passing by the 
local minima, the concept of stochastic update is used in the activation 
dynamics of the network. And for a stochastic update the state of the unit 
is updated using a probability law, which is controlled by a temperature 
parameter (T). At low temperatures, the stochastic update approaches 
the deterministic update, which is dictated by the output function of the unit.
</OL></OL>
<OL><OL>
(e) The state of a neural network with stochastic update is described 
in terms of a probability distribution. The probability distributions of 
the states at thermal equilibrium follow the Boltzmann-Gibb's law 
, namely
<OL><OL><br>
\(P(s_\alpha)=(1/Z)e^{-E_\alpha /T} \qquad(3)\)
</OL></OL>
<br>
where \(E_\alpha\),is the energy of the network in the state \(s_\alpha\) and \(Z\) is the 
partition function given by
<OL><OL><br>
\(Z = \sum\limits_{\alpha}e^{-E_\alpha/T} \qquad(4)\)
</OL></OL><br>
The network is allowed to relax to thermal equilibrium at a given 
temperature (T). Due to stochastic update the state of the network
does not remain constant at thermal equilibrium. But the average 
value of the state of the network remains constant due to stationarity 
of the probabilities \(P(s_\alpha)\) of the states of the network at thermal
equilibrium. The average value of the state vector in given by
<OL><OL><br>
\(s^{av} = \sum\limits_{\alpha} s_\alpha P(s_\alpha) \qquad(5)\)
</OL></OL><br>
(f) At higher temperatures many states are likely to be visited,
irrespective of the energies of those states. Thus the local minima of 
the energy function can be escaped. As the temperature is gradually 
reduced, the states having lower energies will be visited more 
frequently. Finally, at T = 0, the state with the lowest energy will 
have the highest probability. Thus the state corresponding to the 
global minimum of the energy function can be reached, escaping the 
local minima. This method of search for the global minimum of the 
energy function is called simulated annealing. 
Implementation of simulated annealing requires computation of 
stationary probabilities at thermal equilibrium for each temperature
in the annealing schedule. Moreover, the convergence to the global 
minimum is guaranteed only if the temperature parameter is reduced 
slowly starting from a high value initially [Geman and Geman, 1984]. 
The probabilities of the states are computed by collecting the distribution of 
the states after a large number of cycles of updates of the states
of the network at a given temperature. The cycles are repeated until the 
probabilities of states do not change substantially for different sets 
of cycles. Once thermal equilibrium is reached, the temperature 
is changed to the next lower value. Thus the process of 
implementation of simulated annealing is very slow. 
</OL></OL>
<OL><OL>
 (g) In order to speed up the process of simulated annealing, the 
mean-ﬁeld annealing approximation is used [Peterson and Anderson, 
1987], in which the stochastic update of the binary/bipolar units is 
replaced by deterministic analog states [Glauber, 1963]. The basic 
idea of mean-ﬁeld approximation is to replace the ﬂuctuating activation 
values of each unit by its average value. That is \(x_i\) is 
replaced by \(x^{av}\).
<OL><OL><br>
\(x^{av} = (\sum\limits_{j} w_{ij}s_j)^{av}\) \(=\) \(\sum\limits_{j}w_{ij}s_j^{av} \qquad(6)\)
</OL></OL><br>
where \(x^{av}\) represents the expectation in average of the random 
quantities. Likewise, in the average of the state of the \(i^{th}\) unit given
by 
<OL><OL><br>
\(x^{av} = tanh\{(x_i)/T \} \qquad(7)\)
</OL></OL><br>
If \(x_i\) is replaced by \(x^{av}\), we get from above equations 
<OL><OL><br>
\(s_i^{av} = tanh\{\frac{1}{T} \sum\limits_{j} w_{ij} s_j^{av})\} \qquad(8)\)
</OL></OL><br>
The mean-ﬁeld approximation involves solving the following recursive 
equations involving the average values of the states of the units.
<OL><OL><br>
\( s_i^{av}(t+1) = tanh\{\frac{1}{T} \sum\limits_{j=1}^N w_{ij} s_j^{av})\} \) ;     \(i = 1,2, .... ,N \qquad(9)\)
</OL></OL><br>

These are a set of coupled nonlinear deterministic equations. The 
equations are solved iteratively starting with some arbitrary values
\(s_i^{av}(0)\) initially. Once the steady equilibrium values of \(s_i^{av}\) are
obtained, then the temperature is lowered. The next set of average
states at thermal equilibrium are determined using the average state
values of the previous thermal equilibrium condition as the initial
values \(s_i^{av}(0)\) in the equations above for iterative solution. Note that,
due to the deterministic set of equations involved in this computation,
the computation will be much faster than in the case of simulated
annealing. While convergence to global minimum is not
guaranteed in the mean-ﬁeld approximation, it yields good
results [Haykin, 1994]

</P>
        </div>

        </section>

 <!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->

      <section id="experiment-article-section-3">

        <div id="experiment-article-section-3-icon" 
        class="icon">
	<img src="../images/simulation.jpg" />
		</div>

        <div id="experiment-article-section-3-heading" 
        class="heading">
        Illustration 
        </div>

        <div id="experiment-article-section-3-content" 
        class="content">

<h3>Weighted matching problem</h3>

<p>Given a set of N points along with the distances between each pair of points, find an optimal pairing of the points such that the total length of the distances of the pairs is minimum.
</p>
<br>

<h3>Example:</h3> 
<p>Consider N=4 points as shown in Figure 1 (a). The distances between each pair of points is given in Figure 1 (b).</p>



		<table width = "600">
		<tr>
		<td height = "200"><img src="images/prob.png" style="height:90%;width:75%"/></td>
		<td height = "200"><img src="images/wgraph.png" style="height:90%;width:75%"/></td>
		</tr>
		<tr>
		<td align=center><b>Figure 1 (a)</b></td>
		<td align=center><b>Figure 1 (b)</b></td>
		</tr>
               </table>



<p> The possible pairing of points are given in Figures 2 (a) to 2 (c). </p>


<table width = "900">
                <tr>    
                <td height = "200"><img src="images/sol1.png" style="height:90%;width:75%"/></td>
                <td height = "200"><img src="images/sol2.png" style="height:90%;width:75%"/></td>
		<td height = "200"><img src="images/sol3.png" style="height:90%;width:75%"/></td>
                </tr>
                <tr>
		<td align=center><b>Figure 2 (a): </b><em>L=7.2 </em></td>
		<td align=center><b>Figure 2 (b): </b><em>L=12.8 </em></td>
		<td align=center><b>Figure 2 (c): </b><em>L=14.6 </em></td>

                </tr>
               </table>




<p>The total length of the paired links (\( L=\sum\limits_{i\lt j}d_{ij}n_{ij} \)) are 7.2, 12.8 and 14.6, respectively. </p><br>
<p><font color=#FF0000>Therefore, Fig.2a is the solution to this weighted matching problem.</font></p>

        </div>

        </section>


 <!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->

      <section id="experiment-article-section-4">

        <div id="experiment-article-section-4-icon" 
        class="icon">
	<img src="../images/procedure.jpg" />
	</div>

        <div id="experiment-article-section-4-heading" 
        class="heading">
         Procedure 
        </div>

        <div id="experiment-article-section-4-content" 
        class="content">
	<OL><OL><UL>
	<LI><P>Go through the example presented, so as to understand the operation being done in the experiment to optimize the weighted matching problem.</P>
	<LI><P>Select the type of relaxation.</P>
	<LI><P>Select a graph type with given number of nodes along with the location of the nodes in a 2-D plane.</P>
	<LI><P>Click on 'Init' to initialize the graph.</P>
	<LI><P>After observing the set of points generated and the equations used to optimize the weighted graph, click on 'Anneal' to start annealing the network.</P>
	<LI><P>Go through the output of the problem and the output states of nodes after each update.</P>
	</UL></OL></OL>
        </div>

        </section>


 <!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->

      <section id="experiment-article-section-5">

        <div id="experiment-article-section-5-icon" 
        class="icon">
	<img src="../images/simulation.jpg" />
	</div>

        <div id="experiment-article-section-5-heading" 
        class="heading">
        Experiment
        </div>

        <div id="experiment-article-section-5-content" 
        class="content">
<!--
          <p>
                <a href="http://speech.iiit.ac.in/vlabs/weboctave"> Click here to start weboctave.</a>
          </p>
-->
<p>                <IFRAME src="weightedmatching.php" width="1000" height="1200" >     </IFRAME>  </p>

        </div>

        </section>


 <!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->
        <section id="experiment-article-section-6">
      
          <div id="experiment-article-section-6-icon" 
          class="icon">
	    <!-- Enclose the icon image of your lab.-->
	<img src="../images/manual.jpg" />
	  </div>

          <div id="experiment-article-section-6-heading" 
          class="heading">
           Observations 
          </div>

          <div id="experiment-article-section-6-content" 
          class="content">
            <OL><OL><UL>

<LI><P>Deterministic relaxation can get stuck in local minima depending on the starting point. </P>
<LI><P>Stochastic relaxation uses simulated annealing at different temperatures with probablistic update,
which can help the network get out of the local minima, to settle for a deeper minima.</P>
<LI><P>Stochastic relaxation can take time to reach equilibrium. </P>
<LI><P>Mean-field annealing is used to speed up the process. </P>
            </UL></OL></OL>
          </div>

        </section>

	 
 <!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->
        <section id="experiment-article-section-7">
      
          <div id="experiment-article-section-7-icon" 
          class="icon">
	    <!-- Enclose the icon image of your lab.-->
	<img src="../images/manual.jpg" />
	  </div>

          <div id="experiment-article-section-7-heading" 
          class="heading">
           Assignment 
          </div>

          <div id="experiment-article-section-7-content" 
          class="content">
        <p>
<OL>
        <OL>
                <OL>
                        <LI><P> What is a local minima problem in optimization ?</P>
                        <LI><P> How is mean-field annealing applied in the solution of optimization problems?</P>
			<LI><P> Discuss the solution to the Traveling salesman problem using deterministic relaxation and stochastic relaxation. </P>
			<P> Hint: Refer [Yegnanarayana, 1999, pg. 299] and [Wilson and Pawley, 1988]</P>
                </OL>
        </OL>
</OL>

</p>

        
          </div>

        </section>

 
<!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  -->

<section id="experiment-article-section-8">
	<div id="experiment-article-section-8-icon" 
	class="icon">

	<!-- Enclose the icon image of your lab.-->
	<img src="../images/readings.jpg" />
	</div>
	<div id="experiment-article-section-8-heading" 
	class="heading">
	References
	</div>
	<div id="experiment-article-section-8-content"                                                          
	class="content">
<p>
	<OL>
		<OL>
			<UL>
			<LI><P>B. Yegnanarayana, <i>Artificial Neural Networks</i>, New Delhi, India : Prentice-Hall of India, p. 293, 1999.</P>
			<LI><P>C. Peterson and B. Soderberg, "Neural optimization", in <i>The Handbook of Brain Theory and Neural Networks</i> (M.A. Arbib, ed.), Cambridge, 
			MA: MIT Press, pp. 617-621, 1995.</P>
			<LI><P>J.A. Hertz, A. Krogh, and R.G. Palmer, <i>Introduction to the Theory of Neural Computation</i>, New York: Addison-Wesley, 1991.</P>
			<LI><P>B. Muller and J. Reinhardt, <i>Neural Networks: An Introduction, Physics of Neural Networks</i>, New York: Springer-Verlag, 1991. </P>
			<LI><P>A.L. Yuille, "Constrained optimization and the elastic net", in <i>The Handbook of Brain Theory and Neural Networks</i> (M.A. Arbib, ed.), Cambridge, MA. 
			MIT Press, pp. 250-255, 1995.</P>
			<LI><P>S. Geman and D. Geman, "Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images", 
			<i>IEEE Trans. Pattern Analysis and Machine Intelligence</i>, vol. 6, pp. 721-741, 1984.</P>
			<LI><P>N. Metropolis, A.W. Rosenbluth, M.N. Rosenbluth, A.H. Teller, and E. Teller, "Equation of state calculations by fast computing machines", 
			<i>J. Chem. Phy.</i>, vol. 21, no. 6, pp. 1087-1092, 1953.</P>
			<LI><P>C. Peterson and J.R. Anderson, "A mean field theory learning algorithm for neural networks", <i>Complex Systems</i>, .vol. 1, pp. 995-1019, 1987.</P>
			<LI><P>R.J. Glauber, "Time-dependent statistics of the Ising model", <i>J. Math. Phys.</i>, vol. 4, pp. 294-307, 1963.</P>
			<LI><P>S. Haykin, <i>Neural Networks: A Comprehensive Foundation</i>, New York: Macmillan College Publishing Company Inc., 1994.</P>
			</UL>
		</OL>
	</OL>
</p>

</div>

</section>


      </div>


    <!-- An article can have a sidebar that contain related 
    links and additional material (however it is kept optional 
    at this moment) -->
    <aside id="lab-article-sidebar" class="default">
      <!-- put the content that you want to appear in the 
      sidebar -->	
    </aside>


    <!-- Article footer can display related content and 
    additional links -->						
    <footer id="lab-article-footer" class="default">
      <!-- Put the content that you want to appear here -->
    </footer>

  </article>


  <!-- Links to other labs, about us page can be kept the lab 
  footer-->
  <footer id="lab-footer" class="default">
    <!-- Put the content here-->
  </footer>

  <footer id="lab-header" class="heading">
  <!-- Put the content here-->
  <div id="lab-header-heading" class="heading">
  <!-- Write the name of your lab and link it to the home page
  of your lab. -->

  <center>
  <table><tr>
  <td><a href="http://speech.iiit.ac.in/" target=_blank><font size=-3>Developed at the Speech and Vision Lab, IIIT Hyderabad</font></a></td>
  </tr></table>
  </center>
  </div>

  </footer>

</div>		

</body>
</html>
